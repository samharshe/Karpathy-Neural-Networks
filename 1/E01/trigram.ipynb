{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a trigram language model of the same architecture as in the video.\n",
    "### Train a trigram language model: take 2 characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss: did it improve over a bigram model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from string import ascii_lowercase\n",
    "%matplotlib inline\n",
    "words = open('../names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "chars.insert(0, '.')\n",
    "stoi = {s:i for i,s in enumerate(chars)}\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "char_pairs = []\n",
    "for i in '.' + ascii_lowercase:\n",
    "    for j in '.' + ascii_lowercase:\n",
    "        char_pairs.append(i + j)\n",
    "\n",
    "ptoi = {p:i for i,p in enumerate(char_pairs)}\n",
    "itop = {i:p for p,i in ptoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "    for i in range(len(chs[:-2])):\n",
    "        ix1 = ptoi[chs[i] + chs[i+1]]\n",
    "        ix2 = stoi[chs[i+2]]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((729,27), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 0: loss = 2.333828926086426\n",
      "trial 1: loss = 2.3331234455108643\n",
      "trial 2: loss = 2.332425355911255\n",
      "trial 3: loss = 2.331735372543335\n",
      "trial 4: loss = 2.331052780151367\n",
      "trial 5: loss = 2.3303778171539307\n",
      "trial 6: loss = 2.329710006713867\n",
      "trial 7: loss = 2.3290493488311768\n",
      "trial 8: loss = 2.3283963203430176\n",
      "trial 9: loss = 2.3277499675750732\n"
     ]
    }
   ],
   "source": [
    "# gradient descent.\n",
    "for k in range(10):\n",
    "    # forward pass.\n",
    "    xenc = F.one_hot(xs, num_classes=729).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "    print(f'trial {k}: loss = {loss.item()}')\n",
    "    \n",
    "    # backward pass.\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update.\n",
    "    W.data += 100 * -W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ra\n",
      "daqtana\n",
      "eveni\n",
      "ro\n",
      "violinnanifpsuleignfbitenan\n",
      "ju\n",
      "vieli\n",
      "emrnkbyjxwkbellettenerol\n",
      "balyn\n",
      "shws\n",
      "zomazmsjuaylengglnvirkgpzsoxc\n",
      "samacmhhus\n",
      "rack\n",
      "re\n",
      "lor\n",
      "mael\n",
      "bellani\n",
      "finsuukjan\n",
      "becania\n",
      "hae\n",
      "jeven\n",
      "karrineth\n",
      "da\n",
      "sh\n",
      "sahlnzrinarlfur\n",
      "samya\n",
      "don\n",
      "kha\n",
      "yessaun\n",
      "na\n",
      "driyah\n",
      "morig\n",
      "kqlnozcptkmguhana\n",
      "gqw\n",
      "ad\n",
      "kaitps\n",
      "cairairayan\n",
      "mee\n",
      "avyn\n",
      "an\n",
      "jan\n",
      "hhpxzytruwabers\n",
      "asterya\n",
      "ron\n",
      "qus\n",
      "tell\n",
      "habreel\n",
      "al\n",
      "henzwefzdp\n",
      "emillah\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "  out = ['.','.']\n",
    "  while True:\n",
    "    ix = ptoi[''.join(out[-2:])]\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=729).float()\n",
    "    logits = xenc @ W \n",
    "    counts = logits.exp() \n",
    "    p = counts / counts.sum(1, keepdims=True)\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(''.join(out[2:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now have 729 x 27 values to determine. this will be more accurate in the limit, but it takes longer to train well because each cell is seen less often per training iteration, which means that the initial values are stickier. that's why this is so bad right now.  \n",
    "\n",
    "update after a few hundred iterations: loss lower than for the previous model. nice.  \n",
    "\n",
    "now to calculate explicitly what the loss should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((729,27), dtype=torch.float32)\n",
    "for w in words:\n",
    "    chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "    for i in range(len(chs[:-2])):\n",
    "        ix1 = ptoi[chs[i] + chs[i+1]]\n",
    "        ix2 = stoi[chs[i+2]]\n",
    "        N[ix1,ix2] += 1\n",
    "        \n",
    "P = F.normalize(N, p=1, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-498647.7812)\n",
      "nll=tensor(498647.7812)\n",
      "nll/n=tensor(2.1857)\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "    for i in range(len(chs[:-2])):\n",
    "        ix1 = ptoi[chs[i] + chs[i+1]]\n",
    "        ix2 = stoi[chs[i+2]]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n+=1\n",
    "        \n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretty good!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
